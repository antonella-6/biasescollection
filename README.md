# biasescollection
Over time, our understanding of data has continuously evolved, shifting from a static representation of facts to a dynamic medium for exploration, critique, and creativity. In contemporary design practices, data is no longer confined to the quantitative; it is embedded with emotional, temporal, and material qualities, challenging designers to reflect on its multifaceted nature. Generative Artificial intelligence (GenAI), as a tool for generating, interpreting, and embodying data, has further transformed how we engage with and understand the world. Yet, these systems, far from being neutral, reflect and perpetuate societal assumptions, biases, and inequities embedded in their training processes. This workshop ‘Subversive Data Practices: Resisting Bias with Generative AI’, positions generative AI as a lens through which to critically examine the assumptions and biases embedded in society. By reflecting on the synthetic ways AI describes our world, participants will uncover the latent narratives and beliefs inscribed in the datasets and algorithms that shape these technologies. Through practice-based and speculative research, the workshop will explore how generative AI systems can prompt new ways of seeing and understanding societal structures, enabling the development of subversive and bias-resistant data practices. Participants will engage in experimental approaches to interrogate normative assumptions, deconstruct the social narratives reinforced by technological systems, and explore how resilience and resistance can be built into data-driven design. By leveraging generative AI as both a critical and creative tool, the workshop will cultivate practices that challenge biases, foster resilience, and explore alternative futures grounded in collective and reflective interactions with technology. 

As the use of artificial intelligence (AI) in design practices and research become increasingly common and predominant, it raises a series of ethical issues. AI systems are becoming significant in influencing our perception of reality, merging data, algorithms, and social practices in ways that can be hard to predict (de Seta, Pohjonen, Knuutila, 2023). AI algorithms serve not only as innovative tools for creating new solutions but also as extensive databases and ‘objective’ narrators of history, objects, and the social patterns of the physical world (van der Burg et al., 2022). However, despite their objectivity, these algorithms are never neutral. As Salvaggio (2024) points out  "Every AI-generated image is an infographic about the dataset. AI images are data patterns transformed into visuals, revealing stories about that dataset and the human choices that shaped it". Thus, AI algorithms emerge as objective biased describers of the latent assumptions, stereotypes, and worldviews embedded by their human developers within the training datasets and model development processes. Consequently, incorporating AI algorithms into design practices introduces the risk of inherent biases, which can skew results and perpetuate inequalities (Quadflieg et al., 2022), furthering stereotypes and distorting the representation of reality (Nicoletti & Bass 2023, Zhou et al., 2023). In this context, critically examining AI systems through practice-based research methodologies and reflection-in-action approaches becomes crucial (Ferrara, 2024).

Building on the capabilities and potential of Generative AI, which excels in analyzing and identifying social patterns and behaviours while also potentially perpetuating inequalities, misrepresentations, or distortions of physical reality inherent in training datasets, interdisciplinary research in fields such as media studies, design, and digital methods has harnessed GenAI. This research often encodes stereotyped representations of society to reveal existing biases (Luccioni, 2023). An emerging practice involves repurposing GenAI outputs for social research, using qualitative and quantitative methods for further scrutiny. For instance, generating images for analysis has shown that Stable Diffusion frequently amplifies racial and gender disparities, particularly in job occupations (Nicoletti & Bass, 2023). Analysis of over 5,000 images indicates that white males are often depicted in leadership roles, while people of colour are associated with lower-paying jobs or criminal activity.

The depiction of biodiversity across GenAI models also varies by language, model, and context, revealing both consistencies and differences (Colombo, De Gaetano & Niederer, 2023). Language and model choice significantly affect the representation of species and human presence. Seasonal and geographical prompts influence the colour scheme and thematic focus, while ecosystem and continent prompts highlight challenges in accurately depicting biodiversity, often making it stereotypical, decorative, and simplified.
Instead of analyzing collections of generated images, Erik Salvaggio (2023) employs media studies approaches for qualitative interpretations, reflecting cultural, social, economic, and political biases. For example, a generated image of a kissing couple—showing a white heterosexual couple with the man appearing reluctant and distorted—reveals underlying assumptions about gender, intimacy, and representation. Salvaggio suggests that understanding the dataset's origin, content, and collection method is crucial for uncovering the biases encoded in AI-generated images, thereby providing insight into societal norms and values.
