The proposed workshop, ‘Collective Bias-Resistant and Resilient Data Practices through Artificial Intelligence Technologies,’ takes a practice-based approach to exploring these issues, using GenAI as both a critical and creative lens. Participants will examine how generative AI systems reflect societal norms and biases, while also exploring how these systems can be reimagined to challenge and subvert entrenched assumptions.
The workshop begins with an introduction to generative AI as a tool for reflection and critique, illustrating how AI outputs reveal societal biases and inequities. Participants will then engage in dataset archaeology, analyzing curated datasets to uncover patterns of exclusion, stereotypes, and power dynamics. Building on these insights, small groups will experiment with alternative prompts and subversive strategies, exploring how generative AI can disrupt biases and generate alternative narratives. The session concludes with a collective reflection on how these critical practices can inform resilient, collaborative approaches to technology, fostering shared responsibility in questioning and resisting societal inequities. 
This proposal is part of a practice-based doctoral research project investigating how information design can challenge dominant, top-down approaches to AI and data by fostering methods that center communities, amplify marginalized voices, and embrace subversive practices. The research focuses on rethinking how technology interacts with society, shifting the perspective from technology shaping societal narratives to society critically shaping how technology is designed, understood, and used. Through this lens, the project seeks to develop practices that resist biases, question power dynamics, and uncover the stories often excluded from dominant technological discourses.

 
Figure 1: Doctoral research framework exploring how information interaction design shifts from top-down, expert-driven AI practices to bottom-up, community-focused, and subversive approaches that critique societal structures and power dynamics. 
Workshop Schedule
The duration of the workshop will be approximately 2 hours, with some flexibility between activities. Below is the detailed schedule:
15’ - Introduction: Generative AI as a Lens for Inquiry
The workshop will begin by introducing generative AI as a tool for reflecting on societal biases and assumptions. Participants will then explore the workshop’s goals, which center on fostering critical and reflective practices through direct engagement with AI tools. To illustrate these concepts, a brief demonstration will showcase how generative AI outputs can reveal embedded societal stereotypes.
40’ - Dataset Archaeology: Uncovering Embedded Assumptions
Participants will engage in a "dataset archaeology" activity to analyze a curated dataset used in training generative AI models. They will examine the dataset to uncover patterns of bias, stereotypes, or omissions, exploring the societal assumptions and power dynamics embedded within the data. Through guided reflection, they will consider which narratives are prioritized or marginalized, how these patterns shape the AI-generated outputs, and how the biases in the dataset reflect broader societal inequities.
40’ - Subversive Narratives: Bias-Resistant Practices
Small groups will build on the insights from the dataset archaeology to explore alternative narratives and experiment with subversive generative AI prompts. They will consider interventions to resist or subvert the biases identified in the dataset and explore ways to prompt the AI to produce outputs that challenge or reframe societal assumptions. Using generative AI as a critical tool, participants will prototype speculative data practices or reimagine biased outputs.
20’ - Collective Data Futures: Resilience and Resistance
Groups will share their findings, interventions, and AI-generated outputs, reflecting on how data influences societal narratives. A facilitated discussion will explore how generative AI, when critically examined, can serve as a tool for resisting bias and questioning societal assumptions. The session will conclude with reflections on integrating dataset analysis and collective, bias-aware practices into design and research, highlighting the importance of collaboration and shared responsibility in shaping resilient futures.
Workshop Requirements
Lapotop (participants) 
References (Heading 3 style) (one paragraph space after end of main text)

Colombo, G., De Gaetano, C. & Niederer, S. (2023). Prompting For Biodiversity: Visual Research With Generative AI. In Digital Methods Summer School 2023. Retrieved from https://wiki.digitalmethods.net/Dmi/PromptingForBiodiversity 
de Seta, G., Pohjonen, M., & Knuutila, A. (2023, July 15). Synthetic ethnography: Field devices for the qualitative study of generative models. https://doi.org/10.31235/osf.io/zvew4  
Ferrara, E. (2024). Fairness and Bias in Artificial Intelligence: A Brief Survey of Sources, Impacts, and Mitigation Strategies. Sci, 6(1). https://www.mdpi.com/2413-4155/6/1/3 
Luccioni, A. S., Akiki, C., Mitchell, M., & Jernite, Y. (2023). Stable bias: Evaluating societal representations in diffusion models. In 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks. Retrieved from https://openreview.net/attachment?id=qVXYU3F017&name=pdf 
Nicoletti, L., & Bass, D. (2023, June 8). Generative AI takes stereotypes and bias from bad to worse. Bloomberg.com. https://bloomberg.com/graphics/2023-generative-ai-bias/   
Quadflieg, S., Neuburg, K., & Nestler, S. (Eds.). (2022). (Dis)Obedience in Digital Societies: Perspectives on the Power of Algorithms and Data. transcript Verlag. https://directory.doabooks.org/handle/20.500.12854/80788  
Salvaggio, E. (2024, January 12). How to read an AI image — cybernetic forests. Cybernetic Forests. https://www.cyberneticforests.com/news/how-to-read-an-ai-image   
van der Burg, V., Akdag Salah, A., Chandrasegaran, S., and Lloyd, P. (2022) Ceci n’est pas une chaise: Emerging practices in designer-AI collaboration. In Lockton, D., Lenzi, S., Hekkert, P., Oak, A., Sádaba, J., Lloyd, P. (Eds.), Proceedings of the DRS2022: Bilbao, 25 June - 3 July, Bilbao, Spain. https://doi.org/10.21606/drs.2022.653  
Zhou, M., Abhishek, V., & Srinivasan, K. (2023) Bias in Generative AI (Work in Progress). Retrieved from https://www.andrew.cmu.edu/user/ales/cib/bias_in_gen_ai.pdf
